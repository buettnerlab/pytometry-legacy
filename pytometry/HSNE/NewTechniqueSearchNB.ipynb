{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incident-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import anndata\n",
    "import pl as pl\n",
    "import tl as tl\n",
    "import numpy as np\n",
    "import time as time\n",
    "\n",
    "# Load Dataset\n",
    "filelocation = r\"/home/felix/Public/datasets/VBh_converted.h5ad\"\n",
    "adata = anndata.read_h5ad(filelocation)\n",
    "\n",
    "# subsampling\n",
    "sc.pp.subsample(adata, 0.03)\n",
    "\n",
    "# normalizing\n",
    "adata.X = np.arcsinh(adata.X / 10)\n",
    "\n",
    "# calc knn\n",
    "sc.pp.neighbors(adata, n_neighbors=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alternate-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_P(T):\n",
    "    return (T + T.transpose()) / (2 * T.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "matched-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, spdiags\n",
    "from scipy.sparse.linalg import eigs\n",
    "\n",
    "def fresh_calc_T(adata):\n",
    "    # get connectivities from adata\n",
    "    c = adata.obsp['connectivities']\n",
    "\n",
    "    # make sure connectivities are symmetric\n",
    "    assert(len((c - c.T).data) == 0), \"connectivities are not symmetric\"\n",
    "\n",
    "    # row-normalise c to give a transition matrix\n",
    "    T = c.multiply(csr_matrix(1.0 / np.abs(c).sum(1)))\n",
    "\n",
    "    # make sure it's correctly row-normalised\n",
    "    assert(np.allclose(T.sum(1), 1)), \"T is not row-normalised\"\n",
    "\n",
    "    # compute the stationary distribution\n",
    "    #from scipy.sparse.linalg import eigs\n",
    "    D, V = eigs(T.T, which='LM')\n",
    "    pi = V[:, 0]\n",
    "\n",
    "    # make sure pi is entirely real\n",
    "    assert((pi.imag == 0).all()), \"This is not the stationary vector, found imaginary entries\"\n",
    "    pi = pi.real\n",
    "\n",
    "    # make sure all entries have the same sign\n",
    "    assert((pi > 0).all() or (pi < 0).all()), \"This is not the stationary vector, found positive and negative entries\"\n",
    "    pi /= pi.sum()\n",
    "\n",
    "    # check pi is normalised correctly\n",
    "    assert(np.allclose(pi.sum(), 1)), \"Pi is not normalized correctly\"\n",
    "\n",
    "    # put the stationary dist into a diag matrix\n",
    "    Pi = spdiags(pi, 0, pi.shape[0], pi.shape[0])\n",
    "\n",
    "    # finally, check for reversibility of T\n",
    "    assert(np.allclose((Pi @ T - T.T @ Pi).data, 0))\n",
    "    \n",
    "    # list of landmarks\n",
    "    \n",
    "    \n",
    "    return T\n",
    "    \n",
    "t0_new = time.time()\n",
    "T_new = fresh_calc_T(adata)\n",
    "t1_new = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "formal-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to old method for calculating T\n",
    "import multiprocessing as mp\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def _calc_first_T(distances_nn, dim):\n",
    "    p = mp.Pool(mp.cpu_count())\n",
    "    probs = p.map(_helper_method_calc_T, [dist.data for dist in distances_nn])\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "    data = []\n",
    "    for pr in probs:\n",
    "        data.extend(pr)\n",
    "    T = csr_matrix((data, distances_nn.indices, distances_nn.indptr), shape=(dim,dim))\n",
    "    return T\n",
    "\n",
    "def _helper_method_calc_T(dist):\n",
    "    d = dist / np.max(dist)\n",
    "    return softmax((-d ** 2) / _binary_search_sigma(d, len(d)))\n",
    "\n",
    "def _binary_search_sigma(d, n_neigh):\n",
    "    # binary search\n",
    "    sigma = 10  # Start Sigma\n",
    "    goal = np.log(n_neigh)  # log(k) with k being n_neighbors\n",
    "    # Do binary search until entropy ~== log(k)\n",
    "    while True:\n",
    "        ent = entropy(softmax((-d ** 2) / sigma))\n",
    "        # check sigma\n",
    "        if np.isclose(ent, goal):\n",
    "            return sigma\n",
    "        if ent > goal:\n",
    "            sigma *= 0.5\n",
    "        else:\n",
    "            sigma /= 0.5\n",
    "\n",
    "\n",
    "t0_old = time.time()\n",
    "T_old = _calc_first_T(adata.obsp['distances'], len(adata.X))\n",
    "\n",
    "t1_old = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "psychological-budget",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--NEW--\n",
      "shape: (3884, 3884)\n",
      "T num entrys: 104022\n",
      "sum of data: 3884.0000001124954\n",
      "sum first row P 0.00026254706394289895\n",
      "length first row P 29\n",
      "P max 5.957213384852228e-05\n",
      "P min 2.230883490297935e-09\n",
      "sum first row T 0.9999999644933268\n",
      "T max 0.2313786894083023\n",
      "T min 7.353321052505635e-06\n",
      "time: 0.07435131072998047\n",
      "\n",
      "--OLD--\n",
      "shape: (3884, 3884)\n",
      "T num entrys: 73796\n",
      "sum of data: 3883.9999999999927\n",
      "sum first row P 0.00026422856798231334\n",
      "length first row P 29\n",
      "P max 1.3861991955299348e-05\n",
      "P min 6.67671528754798e-06\n",
      "sum first row T 1.0\n",
      "T max 0.05407361214597801\n",
      "T min 0.05178147734568901\n",
      "time: 0.45916032791137695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--NEW--\")\n",
    "print(f\"shape: {np.shape(T_new)}\")\n",
    "print(f\"T num entrys: {len(T_new.data)}\")\n",
    "print(f\"sum of data: {sum(T_new.data)}\")\n",
    "P_new = _calc_P(T_new)\n",
    "print(f\"sum first row P {sum(sum((P_new.getrow(0)).toarray()))}\")\n",
    "print(f\"length first row P {P_new[0].getnnz()}\")\n",
    "print(f\"P max {max(P_new.data)}\")\n",
    "print(f\"P min {min(P_new.data)}\")\n",
    "print(f\"sum first row T {sum(sum((T_new.getrow(0)).toarray()))}\")\n",
    "print(f\"T max {max(T_new.data)}\")\n",
    "print(f\"T min {min(T_new.data)}\")\n",
    "print(f\"time: {t1_new-t0_new}\\n\")\n",
    "\n",
    "print(\"--OLD--\")\n",
    "print(f\"shape: {np.shape(T_old)}\")\n",
    "print(f\"T num entrys: {len(T_old.data)}\")\n",
    "print(f\"sum of data: {sum(T_old.data)}\")\n",
    "P_old = _calc_P(T_old)\n",
    "print(f\"sum first row P {sum(sum((P_old.getrow(0)).toarray()))}\")\n",
    "print(f\"length first row P {P_old[0].getnnz()}\")\n",
    "print(f\"P max {max(P_old.data)}\")\n",
    "print(f\"P min {min(P_old.data)}\")\n",
    "print(f\"sum first row T {sum(sum((T_old.getrow(0)).toarray()))}\")\n",
    "print(f\"T max {max(T_old.data)}\")\n",
    "print(f\"T min {min(T_old.data)}\")\n",
    "print(f\"time: {t1_old-t0_old}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "backed-binding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999644933268\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d38a88127ffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstate_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_events\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "n_events = adata.X.shape[0]\n",
    "init_states = csr_matrix((np.ones(n_events), (range(n_events), range(n_events))))\n",
    "state = init_states[0]#.toarray()[0]\n",
    "\n",
    "\n",
    "\n",
    "state_new = state * T_new\n",
    "state_new = state_new.toarray()[0]\n",
    "print(sum(state_new))\n",
    "choice = np.random.choice(range(n_events), 1, p=state_new)\n",
    "print(choice)\n",
    "\n",
    "\n",
    "state_old = state * T_old\n",
    "state_old = state_old.toarray()[0]\n",
    "print(sum(state_old))\n",
    "choice = np.random.choice(range(n_events), 1, p=state_old)\n",
    "print(choice)\n",
    "\n",
    "\n",
    "for s in enumerate(init_states):\n",
    "    s_new = init_states[s[0]] * T_new\n",
    "    s_new = s_new.toarray()[0]\n",
    "    print(sum(s_new))\n",
    "    choice = np.random.choice(range(n_events), 1, p=s_new)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "anonymous-necklace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_events: 3884\n",
      "lm_old\n",
      "[8, 10, 11, 13, 15, 20, 22, 28, 29, 30]\n",
      "867\n",
      "lm_new\n",
      "[3607 2281  552 ... 2798  190 3686]\n",
      "[0.00025746 0.00025739 0.0002557  ... 0.00026743 0.0002596  0.00025744]\n",
      "3884\n"
     ]
    }
   ],
   "source": [
    "# OLD LANDMARKS\n",
    "def _get_landmarks(T, settings):\n",
    "\n",
    "    n_events = T.shape[0]\n",
    "    proposals = np.zeros(n_events)  # counts how many times point has been reached\n",
    "    landmarks = list()  # list of landmarks\n",
    "    global HELPER_VAR\n",
    "    HELPER_VAR = {'T': T,\n",
    "                  'teta': settings['teta'],\n",
    "                  'beta': settings['beta'],\n",
    "                  'beta_thresh': settings['beta_thresh'],\n",
    "                  'n_events': n_events}\n",
    "    init_states = csr_matrix((np.ones(n_events), (range(n_events), range(n_events))))\n",
    "    p = mp.Pool(mp.cpu_count())\n",
    "    hit_list = p.map(_helper_method_get_landmarks, [state for state in init_states])\n",
    "    p.terminate()\n",
    "    p.join()\n",
    "    # evaluate results\n",
    "    for state_hits in hit_list:  # for every states hit_list\n",
    "        for h in state_hits:  # for every hit in some states hit_list\n",
    "            proposals[h[0]] += h[1]\n",
    "\n",
    "    # collect landmarks\n",
    "    min_beta = settings['beta'] * settings['beta_thresh']\n",
    "    for prop in enumerate(proposals):\n",
    "        # if event has been hit min_beta times, it counts as landmark\n",
    "        if prop[1] > min_beta:\n",
    "            landmarks.append(prop[0])\n",
    "    return landmarks\n",
    "\n",
    "def _helper_method_get_landmarks(state):\n",
    "    for i in range(HELPER_VAR['teta']):\n",
    "        state *= HELPER_VAR['T']\n",
    "    destinations = np.random.choice(range(HELPER_VAR['n_events']), HELPER_VAR['beta'], p=state.toarray()[0])\n",
    "    hits = np.zeros((HELPER_VAR['n_events']))\n",
    "    for d in destinations:\n",
    "        hits[d] += 1\n",
    "    return [(h[0], h[1]) for h in enumerate(hits) if h[1] > 0]\n",
    "\n",
    "##################\n",
    "\n",
    "\n",
    "# NEW LANDMARKS\n",
    "def fresh_calc_lm(T):\n",
    "\n",
    "    # make sure it's correctly row-normalised\n",
    "    assert(np.allclose(T.sum(1), 1)), \"T is not row-normalised\"\n",
    "\n",
    "    # compute the stationary distribution\n",
    "    #from scipy.sparse.linalg import eigs\n",
    "    D, V = eigs(T.T, which='LM')\n",
    "    pi = V[:, 0]\n",
    "\n",
    "    # make sure pi is entirely real\n",
    "    #assert((pi.imag == 0).all()), \"This is not the stationary vector, found imaginary entries\"\n",
    "    pi = pi.real\n",
    "\n",
    "    # make sure all entries have the same sign\n",
    "    #assert((pi > 0).all() or (pi < 0).all()), \"This is not the stationary vector, found positive and negative entries\"\n",
    "    pi /= pi.sum()\n",
    "\n",
    "    # check pi is normalised correctly\n",
    "    assert(np.allclose(pi.sum(), 1)), \"Pi is not normalized correctly\"\n",
    "\n",
    "    # put the stationary dist into a diag matrix\n",
    "    Pi = spdiags(pi, 0, pi.shape[0], pi.shape[0])\n",
    "\n",
    "    # finally, check for reversibility of T\n",
    "    #assert(np.allclose((Pi @ T - T.T @ Pi).data, 0))\n",
    "    \n",
    "    # new: get rid of negative stuff\n",
    "    pi += abs(min(pi))\n",
    "    pi /= sum(pi)\n",
    "    \n",
    "    # list of landmarks\n",
    "    return pi, pi.argsort()\n",
    "\n",
    "##################\n",
    "\n",
    "settings = {'T': T_old,\n",
    "              'teta': 50,\n",
    "              'beta': 100,\n",
    "              'beta_thresh': 1.5,\n",
    "              'n_events': T_old.shape[0]}\n",
    "print(f\"n_events: {settings['n_events']}\")\n",
    "\n",
    "lm_old = _get_landmarks(T_old, settings)\n",
    "print('lm_old')\n",
    "print(lm_old[:10])\n",
    "print(len(lm_old))\n",
    "\n",
    "pi, lm_new = fresh_calc_lm(T_old)\n",
    "print('lm_new')\n",
    "print(lm_new)\n",
    "print(pi)\n",
    "print(len(lm_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "clear-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0005149330587023687\n",
      "0.9999999999999993\n",
      "[1.65632601e-04 4.73292837e-04 2.72518011e-04 ... 7.32019182e-05\n",
      " 3.02488361e-04 4.78332100e-04]\n"
     ]
    }
   ],
   "source": [
    "x = lm_new\n",
    "\n",
    "x += abs(min(x))\n",
    "x = x / sum(x)\n",
    "\n",
    "print(min(x))\n",
    "print(max(x))\n",
    "print(sum(x))\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-porcelain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
